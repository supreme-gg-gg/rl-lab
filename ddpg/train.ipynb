{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9ip_0XlOKy_"
   },
   "outputs": [],
   "source": [
    "! pip install wandb gymnasium tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODnKME3zZ6EH"
   },
   "outputs": [],
   "source": [
    "! apt-get install -y build-essential swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ehq1EzTxZ6il"
   },
   "outputs": [],
   "source": [
    "! pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYKXOpu5NjQE"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgZCUQJvZXXm"
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from replay_buffer import *\n",
    "from networks import *\n",
    "from agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SloYvEaZOScE"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "env = gym.make(ENV_NAME, render_mode=\"human\")\n",
    "agent = DDPGAgent(env, device)\n",
    "\n",
    "if PATH_LOAD is not None:\n",
    "    print(\"loading weights\")\n",
    "    agent.load_models()\n",
    "\n",
    "states, _ = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "noise = np.zeros(agent.actions_dim)\n",
    "while not done:\n",
    "    action = agent.get_action(states, noise, evaluation=True)\n",
    "    new_states, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    score += reward\n",
    "    states = new_states\n",
    "print(f\"Inference score: {score}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt98y_vETAIk"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTjCk83bOUKx"
   },
   "outputs": [],
   "source": [
    "single_env = gym.make(ENV_NAME)\n",
    "agent = DDPGAgent(single_env, device)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    config = {\n",
    "        \"learning_rate_actor\": ACTOR_LR,\n",
    "        \"learning_rate_critic\": CRITIC_LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"architecture\": \"DDPG\",\n",
    "        \"infra\": \"MacOS\",\n",
    "        \"env\": ENV_NAME\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        project=f\"ddpg_{ENV_NAME.lower()}\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(MAX_GAMES)):\n",
    "    start_time = time.time()\n",
    "    states, _ = single_env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    # states, _ = env.reset()  # Vectorized version\n",
    "    # dones = np.array([False] * NUM_ENVS)  # Vectorized version\n",
    "    # score = np.zeros(NUM_ENVS)  # Vectorized version\n",
    "\n",
    "    for t in range(MAX_TIMESTEPS):  # Add timestep condition\n",
    "        actions = agent.get_action(states)  # Will return single action\n",
    "        new_states, rewards, terminated, truncated, _ = single_env.step(actions)\n",
    "        done = terminated or truncated  # Single environment done flag\n",
    "        score += rewards if not done else 0  # Single environment score update\n",
    "\n",
    "        # Store experience in replay buffer (single environment version)\n",
    "        agent.replay_buffer.push(states, actions, rewards, new_states, done)\n",
    "\n",
    "        states = new_states\n",
    "\n",
    "        if agent.replay_buffer.buffer_counter > BATCH_SIZE:\n",
    "            critic_loss, actor_loss, q_value = agent.learn()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores.append(score)  # Changed from extend to append for single score\n",
    "\n",
    "    if ENABLE_WANDB:\n",
    "        wandb.log({\n",
    "            'Game number': i + 1,\n",
    "            'Average reward (last 10 games)': np.mean(scores[-10:]),\n",
    "            'Time taken': round(time.time() - start_time, 2),\n",
    "            'Critic Loss': critic_loss,\n",
    "            'Actor Loss': actor_loss,\n",
    "            'Average Q Value': q_value\n",
    "        })\n",
    "\n",
    "\n",
    "    if (i + 1) % SAVE_FREQUENCY == 0:\n",
    "        print(\"saving...\")\n",
    "        agent.save_models()\n",
    "        print(\"saved\")\n",
    "\n",
    "agent.save_models()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
