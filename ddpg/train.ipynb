{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import *\n",
    "from replay_buffer import *\n",
    "from networks import *\n",
    "from agent import *\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    if args.inference:\n",
    "        env = gym.make(ENV_NAME, render_mode=\"human\")\n",
    "        agent = DDPGAgent(env)\n",
    "\n",
    "        if PATH_LOAD is not None:\n",
    "            print(\"loading weights\")\n",
    "            agent.load_models()\n",
    "        \n",
    "        states, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        noise = np.zeros(agent.actions_dim)\n",
    "        while not done:\n",
    "            action = agent.get_action(states, noise, evaluation=True)\n",
    "            new_states, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            score += reward\n",
    "            states = new_states\n",
    "        print(f\"Inference score: {score}\")\n",
    "        return\n",
    "    \n",
    "    env = gym.make(ENV_NAME)\n",
    "    agent = DDPGAgent(env)\n",
    "    \n",
    "    config = {\n",
    "        \"learning_rate_actor\": ACTOR_LR,\n",
    "        \"learning_rate_critic\": ACTOR_LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"architecture\": \"DDPG\",\n",
    "        \"infra\": \"MacOS\",\n",
    "        \"env\": ENV_NAME\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        project=f\"ddpg_{ENV_NAME.lower()}\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Main training loop\n",
    "    for i in tqdm(range(MAX_GAMES)):\n",
    "        start_time = time.time()\n",
    "        states, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        noise = np.zeros(agent.actions_dim)\n",
    "        while not done:\n",
    "            action = agent.get_action(states, noise)\n",
    "            new_states, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            score += reward\n",
    "            agent.replay_buffer.push(states, action, reward, new_states, done)\n",
    "            agent.learn()\n",
    "            states = new_states\n",
    "\n",
    "        agent.replay_buffer.update_n_games()\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "        wandb.log({'Game number': agent.replay_buffer.n_games, '# Episodes': agent.replay_buffer.buffer_counter, \n",
    "               \"Average reward\": round(np.mean(scores[-10:]), 2), \\\n",
    "                       \"Time taken\": round(time.time() - start_time, 2)})\n",
    "\n",
    "        # Log Q-value estimate on a batch of states and actions from the replay buffer\n",
    "        if agent.replay_buffer.buffer_counter > agent.replay_buffer.batch_size:\n",
    "            # Sample batch for a more representative Q-value\n",
    "            batch_states, batch_actions, _, _, _ = agent.replay_buffer.sample()\n",
    "            with torch.no_grad():\n",
    "                q_values = agent.critic(torch.tensor(batch_states, dtype=torch.float32), torch.tensor(batch_actions, dtype=torch.float32)).mean().detach().item()\n",
    "            wandb.log({\"Average Q-value\": q_values})\n",
    "\n",
    "        if (i + 1) % EVALUATION_FREQUENCY == 0:\n",
    "            states, _ = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            noise = np.zeros(agent.actions_dim) \n",
    "            \n",
    "            while not done:\n",
    "                action = agent.get_action(states, noise, evaluation=True)\n",
    "                new_states, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                score += reward\n",
    "                states = new_states\n",
    "            \n",
    "            wandb.log({'Game number': agent.replay_buffer.n_games, \n",
    "                    '# Episodes': agent.replay_buffer.buffer_counter, \n",
    "                    'Evaluation score': score})\n",
    "            \n",
    "        if (i + 1) % SAVE_FREQUENCY == 0:\n",
    "            print(\"saving...\")\n",
    "            agent.save_models()\n",
    "            print(\"saved\")\n",
    "\n",
    "    agent.save_models()\n",
    "\n",
    "    # Create and log artifact\n",
    "    artifact = wandb.Artifact(name=\"model_saved\", type=\"model\")\n",
    "    artifact.add_file(\"../models/actor.pth\")\n",
    "    artifact.add_file(\"../models/critic.pth\")\n",
    "    artifact.add_file(\"../models/target_critic.pth\")\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"DDPG Training and Inference\")\n",
    "    parser.add_argument('--inference', action='store_true', help='Run inference instead of training')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
